{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use second GPU -- change if you want to use a first one\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "# Add a path to a custom fork of TF-Slim\n",
    "# Get it from here:\n",
    "# https://github.com/warmspringwinds/models/tree/fully_conv_vgg\n",
    "sys.path.append(\"/home/lennon.lin/Repository/github/tensorflow-Model/slim/\")\n",
    "\n",
    "# Add path to the cloned library\n",
    "sys.path.append(\"/home/lennon.lin/Repository/github/tf-image-segmentation/\")\n",
    "\n",
    "checkpoints_dir = '/home/lennon.lin/Checkpoint'\n",
    "log_folder = '/home/lennon.lin/Repository/github/segmentation/log_folder_resnet_101_16s'\n",
    "\n",
    "number_of_epochs = 20\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "resnet_101_v1_checkpoint_path = os.path.join(checkpoints_dir, 'resnet_v1_101.ckpt')\n",
    "\n",
    "from tf_image_segmentation.utils.tf_records import read_tfrecord_and_decode_into_image_annotation_pair_tensors\n",
    "from tf_image_segmentation.models.resnet_v1_101_16s import resnet_v1_101_16s, extract_resnet_v1_101_mapping_without_logits\n",
    "\n",
    "from tf_image_segmentation.utils.pascal_voc import pascal_segmentation_lut\n",
    "from tf_image_segmentation.utils.cityscape import cityscape_Label\n",
    "\n",
    "from tf_image_segmentation.utils.training import get_valid_logits_and_labels\n",
    "\n",
    "from tf_image_segmentation.utils.augmentation import (distort_randomly_image_color,\n",
    "                                                      flip_randomly_left_right_image_with_annotation,\n",
    "                                                      scale_randomly_image_with_annotation_with_fixed_size_output)\n",
    "\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "image_train_size = [384, 384]\n",
    "#number_of_classes = 21\n",
    "#tfrecord_filename = '/home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/recipes/pascal_voc/pascal_augmented_train.tfrecords'\n",
    "tfrecord_filename = '/home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/recipes/cityscape/cityscape_trainid_train.tfrecords'\n",
    "pascal_voc_lut = pascal_segmentation_lut()\n",
    "#class_labels = pascal_voc_lut.keys()\n",
    "class_labels = map(lambda x: x[1] ,filter(lambda x: x[2] != 255 ,cityscape_Label))\n",
    "number_of_classes = len(class_labels) -1\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:36: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn2 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "WARNING:tensorflow:From /home/lennon.lin/Repository/github/tf-image-segmentation/tf_image_segmentation/utils/augmentation.py:40: calling cond (from tensorflow.python.ops.control_flow_ops) with fn1 is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "fn1/fn2 are deprecated in favor of the true_fn/false_fn arguments.\n",
      "INFO:tensorflow:Restoring parameters from /home/lennon.lin/Checkpoint/resnet_v1_101.ckpt\n",
      "Current loss: 2.5214\n",
      "Model saved in file: /home/lennon.lin/Repository/github/segmentation/model_resnet_101_16s.ckpt\n",
      "Current loss: 1.7752\n",
      "Current loss: 1.78213\n",
      "Current loss: 1.961\n",
      "Current loss: 1.50244\n",
      "Current loss: 1.1072\n",
      "Current loss: 0.842412\n",
      "Current loss: 1.49455\n",
      "Current loss: 1.09107\n",
      "Current loss: 1.67083\n",
      "Current loss: 1.07077\n",
      "Current loss: 0.928717\n",
      "Current loss: 0.974274\n",
      "Current loss: 0.666134\n",
      "Current loss: 0.737338\n",
      "Current loss: 0.672874\n",
      "Current loss: 0.720968\n",
      "Current loss: 0.919682\n",
      "Current loss: 0.653202\n",
      "Current loss: 0.715979\n",
      "Current loss: 0.899436\n",
      "Current loss: 0.666087\n",
      "Current loss: 0.588034\n",
      "Current loss: 0.738711\n",
      "Current loss: 0.427495\n",
      "Current loss: 0.75433\n",
      "Current loss: 0.653179\n",
      "Current loss: 0.761312\n",
      "Current loss: 0.724307\n",
      "Current loss: 0.406653\n",
      "Current loss: 0.509123\n",
      "Current loss: 0.507618\n",
      "Current loss: 0.535912\n",
      "Current loss: 0.713401\n",
      "Current loss: 0.557513\n",
      "Current loss: 0.650736\n",
      "Current loss: 0.600262\n",
      "Current loss: 1.04248\n",
      "Current loss: 0.515445\n",
      "Current loss: 0.660842\n",
      "Current loss: 0.794742\n",
      "Current loss: 0.64831\n",
      "Current loss: 0.749453\n",
      "Current loss: 0.637498\n",
      "Current loss: 0.444826\n",
      "Current loss: 1.04228\n",
      "Current loss: 0.49105\n",
      "Current loss: 0.996115\n",
      "Current loss: 0.72047\n",
      "Current loss: 1.09781\n",
      "Current loss: 2.80698\n",
      "Current loss: 2.20306\n",
      "Current loss: 1.45902\n",
      "Current loss: 2.13178\n",
      "Current loss: 4.3477\n",
      "Current loss: 5.31867\n",
      "Current loss: 2.88168\n",
      "Current loss: 5.35075\n",
      "Current loss: 5.92327\n",
      "Current loss: 12.1938\n",
      "Current loss: 11.2056\n",
      "Current loss: 30.6679\n",
      "Current loss: 38.2198\n",
      "Current loss: 147.817\n",
      "Current loss: 217.163\n",
      "Current loss: 290.008\n",
      "Current loss: 219.411\n",
      "Current loss: 872.922\n",
      "Current loss: 2171.21\n",
      "Current loss: 1520.56\n",
      "Current loss: 2902.28\n",
      "Current loss: 12866.0\n",
      "Current loss: 15857.9\n",
      "Current loss: 28584.2\n",
      "Current loss: 39227.1\n",
      "Current loss: 94907.1\n",
      "Current loss: 102916.0\n",
      "Current loss: 118982.0\n",
      "Current loss: 466443.0\n",
      "Current loss: 883034.0\n",
      "Current loss: 598256.0\n",
      "Current loss: 2.19322e+06\n",
      "Current loss: 2.79292e+06\n",
      "Current loss: 4.50597e+06\n",
      "Current loss: 6.51746e+06\n",
      "Current loss: 1.32214e+07\n",
      "Current loss: 2.08251e+07\n",
      "Current loss: 7.25725e+07\n",
      "Current loss: 1.03051e+08\n",
      "Current loss: 8.75207e+07\n",
      "Current loss: 9.00064e+07\n",
      "Current loss: 2.83352e+08\n",
      "Current loss: 8.0659e+08\n",
      "Current loss: 1.94917e+09\n",
      "Current loss: 1.59795e+09\n",
      "Current loss: 2.42005e+09\n",
      "Current loss: 4.28548e+09\n",
      "Current loss: 6.32261e+09\n",
      "Current loss: 1.14837e+10\n",
      "Current loss: 2.48241e+10\n",
      "Current loss: 4.43525e+10\n",
      "Current loss: 6.23644e+10\n",
      "Current loss: 1.00425e+11\n",
      "Current loss: 2.07538e+11\n",
      "Current loss: 2.8846e+11\n",
      "Current loss: 6.15782e+11\n",
      "Current loss: 1.04602e+12\n",
      "Current loss: 1.34425e+12\n",
      "Current loss: 1.78154e+12\n",
      "Current loss: 6.17078e+12\n",
      "Current loss: 1.05548e+13\n",
      "Current loss: 8.14661e+12\n",
      "Current loss: 3.33142e+13\n",
      "Current loss: 5.62781e+13\n",
      "Current loss: 6.59881e+13\n",
      "Current loss: 8.39181e+13\n",
      "Current loss: 3.6206e+14\n",
      "Current loss: 4.95005e+14\n",
      "Current loss: 1.0026e+15\n",
      "Current loss: 1.19771e+15\n",
      "Current loss: 3.14243e+15\n",
      "Current loss: 5.79735e+15\n",
      "Current loss: 5.40115e+15\n",
      "Current loss: 1.13278e+16\n",
      "Current loss: 2.65993e+16\n",
      "Current loss: 8.72753e+16\n",
      "Current loss: 1.91852e+17\n",
      "Current loss: 2.62647e+17\n",
      "Current loss: 2.30064e+17\n",
      "Current loss: 4.28763e+17\n",
      "Current loss: 1.51832e+18\n",
      "Current loss: 1.29755e+18\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n",
      "Current loss: nan\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecord_filename], num_epochs=number_of_epochs)\n",
    "\n",
    "image, annotation = read_tfrecord_and_decode_into_image_annotation_pair_tensors(filename_queue)\n",
    "\n",
    "# Various data augmentation stages\n",
    "image, annotation = flip_randomly_left_right_image_with_annotation(image, annotation)\n",
    "\n",
    "# image = distort_randomly_image_color(image)\n",
    "\n",
    "resized_image, resized_annotation = scale_randomly_image_with_annotation_with_fixed_size_output(image, annotation, image_train_size)\n",
    "\n",
    "\n",
    "resized_annotation = tf.squeeze(resized_annotation)\n",
    "\n",
    "image_batch, annotation_batch = tf.train.shuffle_batch( [resized_image, resized_annotation],\n",
    "                                             batch_size=1,\n",
    "                                             capacity=3000,\n",
    "                                             num_threads=2,\n",
    "                                             min_after_dequeue=1000)\n",
    "\n",
    "\n",
    "# is_training=False here means that we fix the mean and variance\n",
    "# of original model that was trained on Imagenet.\n",
    "# https://github.com/tensorflow/tensorflow/issues/1122\n",
    "upsampled_logits_batch, resnet_v1_101_variables_mapping = resnet_v1_101_16s(image_batch_tensor=image_batch,\n",
    "                                                           number_of_classes=number_of_classes,\n",
    "                                                           is_training=False)\n",
    "\n",
    "\n",
    "valid_labels_batch_tensor, valid_logits_batch_tensor = get_valid_logits_and_labels(annotation_batch_tensor=annotation_batch,\n",
    "                                                                                     logits_batch_tensor=upsampled_logits_batch,\n",
    "                                                                                    class_labels=class_labels)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=valid_logits_batch_tensor,\n",
    "                                                          labels=valid_labels_batch_tensor)\n",
    "\n",
    "# Normalize the cross entropy -- the number of elements\n",
    "# is different during each step due to mask out regions\n",
    "cross_entropy_sum = tf.reduce_mean(cross_entropies)\n",
    "\n",
    "pred = tf.argmax(upsampled_logits_batch, dimension=3)\n",
    "\n",
    "probabilities = tf.nn.softmax(upsampled_logits_batch)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"adam_vars\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(cross_entropy_sum)\n",
    "\n",
    "\n",
    "# Variable's initialization functions\n",
    "resnet_v1_101_without_logits_variables_mapping = extract_resnet_v1_101_mapping_without_logits(resnet_v1_101_variables_mapping)\n",
    "\n",
    "\n",
    "init_fn = slim.assign_from_checkpoint_fn(model_path=resnet_101_v1_checkpoint_path,\n",
    "                                         var_list=resnet_v1_101_without_logits_variables_mapping)\n",
    "\n",
    "global_vars_init_op = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar('cross_entropy_loss', cross_entropy_sum)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_string_writer = tf.summary.FileWriter(log_folder)\n",
    "\n",
    "# Create the log folder if doesn't exist yet\n",
    "if not os.path.exists(log_folder):\n",
    "     os.makedirs(log_folder)\n",
    "    \n",
    "#The op for initializing the variables.\n",
    "local_vars_init_op = tf.local_variables_initializer()\n",
    "\n",
    "combined_op = tf.group(local_vars_init_op, global_vars_init_op)\n",
    "\n",
    "# We need this to save only model variables and omit\n",
    "# optimization-related and other variables.\n",
    "model_variables = slim.get_model_variables()\n",
    "saver = tf.train.Saver(model_variables)\n",
    "\n",
    "\n",
    "with tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(combined_op)\n",
    "    init_fn(sess)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # 10 epochs\n",
    "    for i in xrange(11127 * number_of_epochs):\n",
    "    \n",
    "        cross_entropy, summary_string, _ = sess.run([ cross_entropy_sum,\n",
    "                                                      merged_summary_op,\n",
    "                                                      train_step ])\n",
    "        \n",
    "        print(\"Current loss: \" + str(cross_entropy))\n",
    "        \n",
    "        summary_string_writer.add_summary(summary_string, i)\n",
    "        \n",
    "        if i % 11127 == 0:\n",
    "            save_path = saver.save(sess, \"/home/lennon.lin/Repository/github/segmentation/model_resnet_101_16s.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    save_path = saver.save(sess, \"/home/lennon.lin/Repository/github/segmentation/model_resnet_101_16s.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "summary_string_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
